{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12dded",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_deterministic_ops'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import optax\n",
    "\n",
    "from datetime import datetime\n",
    "from flax import serialization\n",
    "from flax.training import checkpoints\n",
    "from jax import random\n",
    "\n",
    "from cellori.applications.spots import data\n",
    "from cellori.applications.spots import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e8875",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "model_id = 'spots' + '059'\n",
    "model_description = 'Train with 08232022 dataset with new weighted binary cross entropy loss.'\n",
    "models_path = 'models/spots'\n",
    "checkpoint_prefix = model_id + '_checkpoint'\n",
    "dataset_path = 'datasets/rajlab/08232022'\n",
    "random_seed = 0\n",
    "batch_size = 16\n",
    "loss_weights = {\n",
    "    'sl1l': 1,\n",
    "    'bcel': 1,\n",
    "    'invf1': 3\n",
    "}\n",
    "learning_config = {\n",
    "    'schedule': 'exponential_decay',\n",
    "    'init_value': 0.0005,\n",
    "    'transition_steps': 1,\n",
    "    'decay_rate': 0.999\n",
    "}\n",
    "metadata = {\n",
    "    'model_id': model_id,\n",
    "    'model_description': model_description,\n",
    "    'dataset_path': dataset_path,\n",
    "    'date': datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"),\n",
    "    'random_seed': str(random_seed),\n",
    "    'batch_size': str(batch_size),\n",
    "    'loss_weights': loss_weights,\n",
    "    'learning_config': learning_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e145ad9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate paths\n",
    "toc_path = os.path.join(models_path, 'toc.csv')\n",
    "model_path = os.path.join(models_path, model_id)\n",
    "metadata_path = os.path.join(model_path, model_id + '_metadata')\n",
    "batch_metrics_log_path = os.path.join(model_path, model_id + '_batch_metrics_log')\n",
    "epoch_metrics_log_path = os.path.join(model_path, model_id + '_epoch_metrics_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4023b88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and testing datasets\n",
    "ds = data.load_datasets(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490e9a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create train state\n",
    "rng = random.PRNGKey(random_seed)\n",
    "learning_rate = optax.exponential_decay(learning_config['init_value'], \n",
    "                                        learning_config['transition_steps'],\n",
    "                                        learning_config['decay_rate'])\n",
    "state = training.create_train_state(rng, learning_rate)\n",
    "\n",
    "# Check and load in previous train state\n",
    "if os.path.isdir(model_path):\n",
    "    \n",
    "    state = checkpoints.restore_checkpoint(model_path, state, prefix=checkpoint_prefix)\n",
    "    with open(batch_metrics_log_path, 'r') as f_batch_metrics_log:\n",
    "        batch_metrics_log = json.load(f_batch_metrics_log)\n",
    "    with open(epoch_metrics_log_path, 'r') as f_epoch_metrics_log:\n",
    "        epoch_metrics_log = json.load(f_epoch_metrics_log)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    toc_entry = pd.json_normalize(metadata, sep='_')\n",
    "    if os.path.isfile(toc_path):\n",
    "        toc = pd.read_csv(toc_path, index_col=0)\n",
    "        toc = pd.concat((toc, toc_entry), ignore_index=True)\n",
    "    else:\n",
    "        toc = toc_entry\n",
    "    toc.to_csv(toc_path)\n",
    "    \n",
    "    os.makedirs(model_path)\n",
    "    with open(metadata_path, 'w') as f_metadata:\n",
    "        json.dump(metadata, f_metadata, indent=4)\n",
    "        \n",
    "    batch_metrics_log = []\n",
    "    epoch_metrics_log = []\n",
    "    with open(batch_metrics_log_path, 'w') as f_batch_metrics_log:\n",
    "        json.dump(batch_metrics_log, f_batch_metrics_log, indent=4)\n",
    "    with open(epoch_metrics_log_path, 'w') as f_epoch_metrics_log:\n",
    "        json.dump(epoch_metrics_log, f_epoch_metrics_log, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc192f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "epoch_count = len(epoch_metrics_log)\n",
    "for epoch in range(epoch_count + 1, epoch_count + num_epochs + 1):\n",
    "\n",
    "    # Run an optimization step over a training batch\n",
    "    state, batch_metrics, epoch_metrics = training.train_epoch(epoch, state, ds['train'], ds['valid'], batch_size, loss_weights, learning_rate)\n",
    "\n",
    "    batch_metrics_log += batch_metrics\n",
    "    epoch_metrics_log += [epoch_metrics]\n",
    "    \n",
    "    checkpoints.save_checkpoint(model_path, state, epoch, prefix=checkpoint_prefix, keep_every_n_steps=10)\n",
    "    with open(batch_metrics_log_path, 'w') as f_batch_metrics_log:\n",
    "        json.dump(batch_metrics_log, f_batch_metrics_log, indent=4)\n",
    "    with open(epoch_metrics_log_path, 'w') as f_epoch_metrics_log:\n",
    "        json.dump(epoch_metrics_log, f_epoch_metrics_log, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1df4a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "losses = [(epoch_metrics['loss'], epoch_metrics['val_loss']) for epoch_metrics in epoch_metrics_log]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "ax.plot(losses, label=('Training', 'Validation'))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc869802",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "variables = {'params': state.params, 'batch_stats': state.batch_stats}\n",
    "bytes_output = serialization.to_bytes(variables)\n",
    "\n",
    "with open(os.path.join(model_path, model_id + '_model'), 'wb') as f_model:\n",
    "    f_model.write(bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33662b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}